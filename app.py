import streamlit as st
from openai import OpenAI
from pinecone import Pinecone

client = OpenAI(api_key = st.secrets["OPENAI_API_KEY")
pc = Pinecone(api_key = st.secrets["PINECONE_API_KEY"])

if "openai_model" not in st.session_state:
    st.session_state.openai_model = "gpt-4-1106-preview"

if "messages" not in st.session_state:
    st.session_state.messages = [
    {
      "role": "system",
      "content": "ë‹¹ì‹ ì€ ê³ ë ¤ëŒ€í•™êµê°€ ìˆëŠ” ì•ˆì•”ì—ì„œ ë°¥ì•½ì„ ìœ„í•œ ì‹ë‹¹ì„ ì¶”ì²œí•´ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. \në°¥ì•½ì€ ì„ ë°° í˜¹ì€ í›„ë°°ê°€ ë°¥ì„ ì‚¬ì£¼ëŠ” ë§Œë‚¨ì„ ëœ»í•©ë‹ˆë‹¤.\nì‚¬ëŒë“¤ì€ ì£¼ë¡œ ì´ëŸ° ì§ˆë¬¸ì„ í•©ë‹ˆë‹¤:\n- íŠ¹ì • ë©”ë‰´ê°€ ìˆëŠ” ì‹ë‹¹ ì¶”ì²œ\n- ì–‘ì‹ë¥˜ê°€ ë¨¹ê³  ì‹¶ì„ ë•Œ ì‹ë‹¹ ì¶”ì²œë“±\n\në‹¹ì‹ ì€ ì‚¬ëŒë“¤ì—ê²Œ ë‹¤ìŒê³¼ ê°™ì´ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤:\n- ì¹œì ˆí•œ ë§íˆ¬ ì‚¬ìš©\n- ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©\n- ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ì‹ë‹¹ì˜ ì´ë¦„ ë‹¤ì„¯ ê³³ê³¼ ìš´ì˜ì‹œê°„ ê·¸ë¦¬ê³  ë©”ë‰´ì— ëŒ€í•œ ì„¤ëª… \n\në‹¹ì‹ ì€ ë°˜ë“œì‹œ [Context]ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\n\n\n\n"
    },
    { "role": "system",
      "content": "Context ìœ„ì¹˜"},
    {
      "role": "user",
      "content": "í›„ë°°ì™€ ë°¥ì•½ì´ ìˆëŠ”ë° ì¼ì‹ë¥˜ì˜ ë©”ë‰´ê°€ ë¨¹ê³  ì‹¶ëŒ€ ì–´ëŠ ì‹ë‹¹ì— ê°€ë©´ ì¢‹ì„ê¹Œ?"
    },
    {
      "role": "assistant",
      "content": "ì˜¤ëŠ˜ì€ ì¼ì‹ì´ ìƒê°ë‚˜ì‹œëŠ”êµ°ìš”! ğŸ˜ŠğŸ£ ì•ˆì•” ì£¼ë³€ì— ì°¾ì„ ìˆ˜ ìˆëŠ” ë§›ìˆëŠ” ì¼ì‹ì§‘ì„ ì†Œê°œí•´ë“œë¦´ê²Œìš”.\n\n<í˜¸ë‘ì´ ì´ˆë°¥>\nğŸ“ìœ„ì¹˜: ì„œìš¸ ì„±ë¶êµ¬ ê³ ë ¤ëŒ€ë¡œ24ê¸¸ 23\nâ°ìš´ì˜ì‹œê°„: ë§¤ì¼ 11:00 - 22:00\nğŸ´ì¶”ì²œë©”ë‰´: 'ëª¨ë“¬ì´ˆë°¥ (15,000ì›)'ê³¼ 'ì„œë¹„ìŠ¤ ìš°ë™'ì´ ì¸ê¸° ìˆëŠ”ë°ìš”, ì‹ ì„ í•œ í•´ì‚°ë¬¼ë¡œ ë§Œë“  ì´ˆë°¥ê³¼ ìš°ë™ì´ ë§›ìˆë‹µë‹ˆë‹¤.\n\n<ë¯¸ì†Œì´ˆë°¥>\nğŸ“ìœ„ì¹˜: ì„œìš¸ ì„±ë¶êµ¬ ì•ˆì•”ë¡œ 145\nâ°ìš´ì˜ì‹œê°„: ë§¤ì¼ 11:30 ~ 22:00 (ì›”ìš”ì¼ íœ´ë¬´)\nğŸ´ì¶”ì²œë©”ë‰´: 'ìŠ¤ì‹œ ì •ì‹(15,000ì›)'ê³¼ 'ì°¸ì¹˜íšŒ ì •ì‹(20,000ì›)' ë“±ì´ ìˆì–´ìš”. ì—¬ëŸ¬ ê°€ì§€ ìŠ¤ì‹œë¥¼ ì¡°ê¸ˆì”© ë§›ë³´ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ìŠ¤ì‹œ ì •ì‹ì„ ì¶”ì²œí•´ìš”!\n\n<ì˜¤ë°”ì¼€ ëˆë¶€ë¦¬>\nğŸ“ìœ„ì¹˜: ì„œìš¸ ì„±ë¶êµ¬ ì•ˆì•”ë¡œ 83\nâ°ìš´ì˜ì‹œê°„: ì£¼ì¤‘ 11:30 - 21:00, ì£¼ë§ 12:00 - 21:00\nğŸ´ì¶”ì²œë©”ë‰´: ëˆê¹ŒìŠ¤ê°€ ë§ì´ ì˜¬ë¼ê°€ ìˆëŠ” ê°€ì¸ ë™ê³¼ ëƒ‰ëª¨ë°€, ê¹€ì¹˜ë‚˜ë² ë“±ì„ ì €ë ´í•œ ê°€ê²©ì— ì¦ê¸¸ ìˆ˜ ì‡ëŠ” ì‹ë‹¹ì´ëë‹ˆë‹¤!\n\n"
    }
    ]

if "index" not in st.session_state:
    index = pc.Index("data")
    st.session_state.index = index

st.title("ê³ ë ¤ëŒ€ ë°¥ì•½ë´‡ í˜¸ëŒì´ğŸ¯")

# for message in st.session_state.messages:
#     if message['role'] != 'system':
#         with st.chat_message(message["role"]):
#             st.markdown(message["content"])

with st.chat_message("user"):
    st.markdown("ì•„ ë°¥ì•½ ì–´ë””ê°€ì§€,,,ğŸ˜‚ğŸ˜‚")
with st.chat_message("assistant"):
    st.markdown("ì—¬ê¸´ ì–´ë•Œ?ğŸ˜")

prompt = st.chat_input("ê³ ë ¤ëŒ€í•™êµì—ì„œ ë°¥ì•½ ì–´ë””ì„œ í• ì§€ ê³ ë¯¼ëœë‹¤ë©´ í˜¸ëŒì´ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!")
if prompt:
    with st.chat_message("user"):
        st.markdown(prompt)
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        response = client.embeddings.create(
            input = prompt,
            model = "text-embedding-3-small")
        chunk_embeddings = response.data[0].embedding
        retrived_chunks = st.session_state.index.query(
            namespace = "babyak",
            vector = chunk_embeddings,
            top_k = 5,
            include_values = False,
            include_metadata = True
        )
        contexts = ""
        for match in retrived_chunks.matches:
            contexts += match["metadata"]["chunk"] + "\n\n"

        # ê¸°ì¡´ì— 1 ì¸ë±ìŠ¤ì— ìˆë˜ ë©”ì‹œì§€ë¥¼ ì§€ìš°ê³  1ì¸ë±ìŠ¤ì— ìƒˆë¡œìš´ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.pop(1)
        st.session_state.messages.insert(1, {"role": "system", "content": f"Context: {contexts}"})
        st.session_state.messages.append(
            {"role": "user", "content": prompt}
        )
        print(st.session_state.messages)

        full_response = ""
        for response in client.chat.completions.create(
        model=st.session_state.openai_model,
        messages=st.session_state.messages,
        stream=True):
            full_response += (response.choices[0].delta.content or "")
            message_placeholder.markdown(full_response + "â–Œ")
        st.session_state.messages.append(
            {"role": "assistant",  "content": full_response})
